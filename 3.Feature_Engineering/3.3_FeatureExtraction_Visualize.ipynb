{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_JslPhsxK91a"
      },
      "source": [
        "# Importing packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "from scipy.stats import kurtosis\n",
        "from scipy.signal import lfilter\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, scale\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "#from nitime.algorithms.autoregressive import AR_est_YW\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import hiplot as hip\n",
        "sys.path.append('..')\n",
        "sys.path.append('../utils/')\n",
        "from utils.EDA import*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "plt.style.use('default')\n",
        "\n",
        "plt.rcParams.update({\n",
        "    'font.size': 20,\n",
        "    'axes.linewidth': 2,\n",
        "    'axes.titlesize': 20,\n",
        "    'axes.edgecolor': 'black',\n",
        "    'axes.labelsize': 18,\n",
        "    'axes.grid': True,\n",
        "    'lines.linewidth': 1.5,\n",
        "    'lines.markersize': 6,\n",
        "    'figure.figsize': (20, 8),\n",
        "    'xtick.labelsize': 16,\n",
        "    'ytick.labelsize': 16,\n",
        "    'font.family': 'Times New Roman',\n",
        "    'legend.fontsize': 13,\n",
        "    'legend.framealpha': 0.8,\n",
        "    'legend.edgecolor': 'black',\n",
        "    'legend.shadow': False,\n",
        "    'legend.fancybox': True,\n",
        "    'legend.frameon': True,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "metadata": {}
      },
      "outputs": [],
      "source": [
        "path_to_dataset = \"Datasets/OP6_Features.parquet\"\n",
        "df = pd.read_parquet(path_to_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualizing the data after the FE step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_columns = ['Y_Rolling Variance','Y_Rolling Energy Entropy','X_D3','Y_D1', 'Z_D2','Z_Rolling Energy Entropy','Y_Rolling Median']\n",
        "\n",
        "plot_scatter_matrix_FE(df, machine='M02', process='OP06', cols=selected_columns, sample_frac=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_vars = ['X_axis','Y_axis','Z_axis','Machine','Label'] \n",
        "visualize_with_hiplot(df[plot_vars].sample(frac=0.01,random_state=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_vars = ['Z_D3', 'Z_D2', 'Y_D3', 'Y_D2', 'X_D3','Machine','Label','Z_Rolling Energy Entropy', 'Y_Rolling RMS'] \n",
        "visualize_with_hiplot(df[plot_vars].sample(frac=0.01,random_state=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Z_D3, Z_D2, Y_D3, Y_D2, X_D3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "features = ['X_axis', 'X_Rolling Mean', 'X_Rolling Std', 'X_Rolling Max',\n",
        "       'X_Rolling Min', 'X_Rolling Median', 'X_Rolling Variance',\n",
        "       'X_Rolling Skewness', 'X_Rolling Kurtosis', 'X_Rolling RMS',\n",
        "       'X_Rolling Impulse Factor', 'X_Rolling Margin Factor', 'Y_axis',\n",
        "       'Y_Rolling Mean', 'Y_Rolling Std', 'Y_Rolling Max', 'Y_Rolling Min',\n",
        "       'Y_Rolling Median', 'Y_Rolling Variance', 'Y_Rolling Skewness',\n",
        "       'Y_Rolling Kurtosis', 'Y_Rolling RMS', 'Y_Rolling Impulse Factor',\n",
        "       'Y_Rolling Margin Factor', 'Z_axis', 'Z_Rolling Mean', 'Z_Rolling Std',\n",
        "       'Z_Rolling Max', 'Z_Rolling Min', 'Z_Rolling Median',\n",
        "       'Z_Rolling Variance', 'Z_Rolling Skewness', 'Z_Rolling Kurtosis',\n",
        "       'Z_Rolling RMS', 'Z_Rolling Impulse Factor', 'Z_Rolling Margin Factor',\n",
        "       'X_Rolling Energy', 'X_Rolling Energy Entropy',\n",
        "       'X_Rolling Normalized Energy', 'Y_Rolling Energy',\n",
        "       'Y_Rolling Energy Entropy', 'Y_Rolling Normalized Energy',\n",
        "       'Z_Rolling Energy', 'Z_Rolling Energy Entropy',\n",
        "       'Z_Rolling Normalized Energy', 'X_D1', 'X_D2', 'X_D3', 'X_A3', 'Y_D1',\n",
        "       'Y_D2', 'Y_D3', 'Y_A3', 'Z_D1', 'Z_D2', 'Z_D3', 'Z_A3', 'X_Jerk',\n",
        "       'Y_Jerk', 'Z_Jerk']\n",
        "X = df[features]\n",
        "\n",
        "# Standardizing the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_scaled = pd.DataFrame(X_scaled, columns=features)\n",
        "\n",
        "# Adding labels back for later use in plotting\n",
        "#X_scaled = pd.concat([X_scaled, df1['Label']], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_pca_cumulative_variance(X_scaled, n_components=None):\n",
        "    \"\"\"\n",
        "    Fit a PCA model to the given scaled data and plot the cumulative explained variance.\n",
        "\n",
        "    Parameters:\n",
        "    - X_scaled: The scaled input data (e.g., a NumPy array or DataFrame).\n",
        "    - n_components: Number of PCA components to consider (default is None, which means all).\n",
        "    \"\"\"\n",
        "    # Initialize PCA model\n",
        "    pca = PCA(n_components=n_components)\n",
        "    \n",
        "    # Fit PCA model to the scaled data\n",
        "    pca.fit(X_scaled)\n",
        "    \n",
        "    # Calculate the cumulative explained variance\n",
        "    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "    \n",
        "    # Create a plot of the cumulative explained variance\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "    plt.xlabel('Number of Components')\n",
        "    plt.ylabel('Cumulative Explained Variance')\n",
        "    plt.title('PCA - Cumulative Variance Explained by Components')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_pca_cumulative_variance(X_scaled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def pca_analysis_with_inverse_plot(X_scaled, n_components, feature_names):\n",
        "    \"\"\"\n",
        "    Perform PCA, plot a heatmap showing the contributions of each feature to the principal components,\n",
        "    and return the DataFrame of the inverted PCA transformation.\n",
        "\n",
        "    Parameters:\n",
        "    - X_scaled: The scaled input data (NumPy array or DataFrame).\n",
        "    - n_components: Number of PCA components to retain.\n",
        "    - feature_names: List of original feature names.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame of the inverted PCA transformation approximating original features.\n",
        "    \"\"\"\n",
        "    pca = PCA(n_components=n_components)\n",
        "\n",
        "    # Fit PCA and transform the scaled data into principal components\n",
        "    principal_components = pca.fit_transform(X_scaled)\n",
        "\n",
        "    # Inverse transform to approximate the original features\n",
        "    inverted_data = pca.inverse_transform(principal_components)\n",
        "    inverted_df = pd.DataFrame(inverted_data, columns=feature_names)\n",
        "\n",
        "    # Identify the features contributing the most to each principal component\n",
        "    loading_matrix = np.abs(pca.components_)\n",
        "    top_contributors = {}\n",
        "    for idx, component in enumerate(loading_matrix):\n",
        "        top_features_idx = component.argsort()[::-1]  # Sort in descending order\n",
        "        top_features = [feature_names[i] for i in top_features_idx]\n",
        "        top_contributors[f'PC{idx + 1}'] = top_features\n",
        "\n",
        "    # Create a DataFrame of top feature contributions\n",
        "    contributions_df = pd.DataFrame.from_dict(top_contributors, orient='index', columns=[f'Top_Feature_{i + 1}' for i in range(len(feature_names))])\n",
        "\n",
        "    return inverted_df, contributions_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_components = 40  # Choose the desired number of principal components\n",
        "inverted_df, contributions_df = pca_analysis_with_inverse_plot(X_scaled, n_components, feature_names = features)\n",
        "print(\"Approximated Original Features (Inverted PCA):\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "çakçlkdçlkdçdç"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "inverted_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTop Feature Contributions for Each Principal Component:\")\n",
        "contributions_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_features_series = pd.Series(contributions_df.values.flatten())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Apply PCA and retain the first 30 principal components\n",
        "n_components = 40\n",
        "pca = PCA(n_components=n_components)\n",
        "principal_components = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Create a DataFrame for the principal components\n",
        "pca_columns = [f'PC{i+1}' for i in range(n_components)]\n",
        "pca_df = pd.DataFrame(principal_components, columns=pca_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pca_df = pd.concat[[pca_df,]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform the inverse transformation to approximate the original data\n",
        "reconstructed_data = pca.inverse_transform(principal_components)\n",
        "\n",
        "# Convert the reconstructed data back to a DataFrame\n",
        "reconstructed_df = pd.DataFrame(reconstructed_data, columns=features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the Mean Absolute Error (MAE)\n",
        "mae = mean_absolute_error(X_scaled, reconstructed_data)\n",
        "# Print the MAE\n",
        "print(f\"Reconstruction Mean Absolute Error (MAE): {mae}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Undoing the standardizetian\n",
        "reconstructed_original_scale = scaler.inverse_transform(reconstructed_data)\n",
        "reconstructed_original_df = pd.DataFrame(reconstructed_original_scale, columns=features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reconstructed_original_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "reconstructed_original_df = pd.concat([df[['Time', 'Month', 'Year', 'Machine', 'Process', 'Label', 'Unique_Code',\n",
        "       'Period']],reconstructed_original_df], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_columns = ['Y_Rolling Variance','Y_Rolling Energy Entropy','X_D3','Y_D1', 'Z_D2','Z_Rolling Energy Entropy','Y_Rolling Median']\n",
        "\n",
        "plot_scatter_matrix_FE(df, machine='M01', process='OP06', cols=selected_columns, sample_frac=0.1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "selected_columns = ['Y_Rolling Variance','Y_Rolling Energy Entropy','X_D3','Y_D1', 'Z_D2','Z_Rolling Energy Entropy','Y_Rolling Median']\n",
        "\n",
        "plot_scatter_matrix_FE(reconstructed_original_df, machine='M01', process='OP06', cols=selected_columns, sample_frac=0.1, random_state=42)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "chord-bragato.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('gputest3')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "vscode": {
      "interpreter": {
        "hash": "a8a0b88a388d3e076f2a934657bedc7e70d212269d14ee7385687e102196c370"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}

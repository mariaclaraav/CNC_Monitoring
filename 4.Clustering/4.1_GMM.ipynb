{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"metadata":{},"slideshow":{"slide_type":"skip"}},"outputs":[],"source":["# Configuration for suppressing warnings\n","import warnings\n","warnings.filterwarnings('ignore', category=DeprecationWarning)  # Suppress specific categories as needed\n","\n","# Importing standard libraries and configuring path\n","import sys\n","sys.path.append('..')\n","sys.path.append('../utils/')\n","\n","# Importing third-party libraries for data manipulation, machine learning, and visualization\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import hiplot as hip\n","import matplotlib.pyplot as plt\n","from matplotlib.colors import BoundaryNorm\n","from sklearn.decomposition import PCA\n","from sklearn.preprocessing import StandardScaler, scale\n","from sklearn.cluster import KMeans\n","from sklearn.mixture import GaussianMixture\n","from tqdm import tqdm\n","from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score, silhouette_score\n","from sklearn.datasets import make_blobs\n","import umap\n","\n","# Importing Plotly for interactive plotting\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","# Importing local utilities/modules, assuming these are located in the 'utils' directory\n","from utils.EDA import *\n","from utils.Clustering import *\n","\n","# IPython specific configuration to set the backend for rendering high-resolution images in Jupyter notebooks\n","%config InlineBackend.figure_formats = ['retina']"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["plt.style.use('default')\n","\n","plt.rcParams.update({\n","    'font.size': 20,\n","    'axes.linewidth': 2,\n","    'axes.titlesize': 20,\n","    'axes.edgecolor': 'black',\n","    'axes.labelsize': 18,\n","    'axes.grid': True,\n","    'lines.linewidth': 1.5,\n","    'lines.markersize': 6,\n","    'figure.figsize': (20, 8),\n","    'xtick.labelsize': 16,\n","    'ytick.labelsize': 16,\n","    'font.family': 'Times New Roman',\n","    'legend.fontsize': 13,\n","    'legend.framealpha': 0.8,\n","    'legend.edgecolor': 'black',\n","    'legend.shadow': False,\n","    'legend.fancybox': True,\n","    'legend.frameon': True,\n","})"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["path_to_dataset = \"../3.Feature_Engineering/Datasets/OP6_Features.parquet\"\n","df = pd.read_parquet(path_to_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["### Create sub-categories of the data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#variance_col = ['X_Rolling Variance', 'X_Rolling RMS', 'Y_Rolling Variance', 'Y_Rolling RMS','_Rolling Variance', 'Z_Rolling RMS']\n","\n","variance_col = ['X_Rolling RMS', 'Y_Rolling RMS','Z_Rolling RMS']\n","\n","non_dim_col = ['X_Rolling Impulse Factor','X_Rolling Margin Factor', 'Y_Rolling Impulse Factor','Y_Rolling Margin Factor',\n","               'Z_Rolling Impulse Factor','Z_Rolling Margin Factor']\n","\n","wavelet_col = ['X_D1', 'X_D2', 'X_D3', 'X_A3', 'Y_D1','Y_D2', 'Y_D3', 'Y_A3', 'Z_D1', 'Z_D2', 'Z_D3', 'Z_A3']\n","\n","mean_col = ['X_Rolling Mean','X_Rolling Median', 'Y_Rolling Mean','Y_Rolling Median',\n","            'Z_Rolling Mean','Z_Rolling Median']\n","\n","min_max_col = ['X_Rolling Max','X_Rolling Min', 'Y_Rolling Max','Y_Rolling Min', 'Z_Rolling Max']\n","\n","original_col = ['X_Axis','Y_Axis','Z_axis', 'X_Jerk','Y_Jerk','Z_Jerk']\n","\n","energy_col = ['X_Rolling Energy', 'X_Rolling Energy Entropy','Y_Rolling Energy',\n","       'Y_Rolling Energy Entropy',\n","       'Z_Rolling Energy', 'Z_Rolling Energy Entropy']\n","\n","stat_col = ['X_Rolling Skewness', 'X_Rolling Kurtosis','Y_Rolling Skewness', 'Y_Rolling Kurtosis', 'Z_Rolling Skewness', 'Z_Rolling Kurtosis']"]},{"cell_type":"markdown","metadata":{},"source":["- To avoid redundant features we will identify linesar correlation and remove features that are 90% correlated or more"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["col = ['X_axis', 'X_Rolling Mean', 'X_Rolling Std', 'X_Rolling Max',\n","       'X_Rolling Min', 'X_Rolling Median', 'X_Rolling Variance',\n","       'X_Rolling Skewness', 'X_Rolling Kurtosis', 'X_Rolling RMS',\n","       'X_Rolling Impulse Factor', 'X_Rolling Margin Factor', 'Y_axis',\n","       'Y_Rolling Mean', 'Y_Rolling Std', 'Y_Rolling Max', 'Y_Rolling Min',\n","       'Y_Rolling Median', 'Y_Rolling Variance', 'Y_Rolling Skewness',\n","       'Y_Rolling Kurtosis', 'Y_Rolling RMS', 'Y_Rolling Impulse Factor',\n","       'Y_Rolling Margin Factor', 'Z_axis', 'Z_Rolling Mean', 'Z_Rolling Std',\n","       'Z_Rolling Max', 'Z_Rolling Min', 'Z_Rolling Median',\n","       'Z_Rolling Variance', 'Z_Rolling Skewness', 'Z_Rolling Kurtosis',\n","       'Z_Rolling RMS', 'Z_Rolling Impulse Factor', 'Z_Rolling Margin Factor',\n","       'X_Rolling Energy', 'X_Rolling Energy Entropy',\n","       'X_Rolling Normalized Energy', 'Y_Rolling Energy',\n","       'Y_Rolling Energy Entropy', 'Y_Rolling Normalized Energy',\n","       'Z_Rolling Energy', 'Z_Rolling Energy Entropy',\n","       'Z_Rolling Normalized Energy', 'X_D1', 'X_D2', 'X_D3', 'X_A3', 'Y_D1',\n","       'Y_D2', 'Y_D3', 'Y_A3', 'Z_D1', 'Z_D2', 'Z_D3', 'Z_A3', 'X_Jerk',\n","       'Y_Jerk', 'Z_Jerk']\n","\n","correlation_matrix = df[col].corr()\n","\n","# Find pairs of columns with high correlation\n","high_corr_pairs = {}\n","for col in correlation_matrix.columns:\n","    for index in correlation_matrix.index:\n","        if (correlation_matrix.at[index, col] >= 0.8) and (index != col):  # High correlation and excluding the main diagonal\n","            if (index, col) not in high_corr_pairs and (col, index) not in high_corr_pairs:\n","                high_corr_pairs[(index, col)] = correlation_matrix.at[index, col]\n","\n","# Display the pairs of columns and their correlation coefficients\n","for pair, corr_value in high_corr_pairs.items():\n","    print(f\"Pair: {pair[0]}, {pair[1]} - Correlation: {corr_value}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_vars = ['Z_D3', 'Z_D2', 'Y_D3', 'Y_D2', 'X_D3','Machine','Label', 'Y_Rolling Energy Entropy'] \n","visualize_with_hiplot(df[plot_vars].sample(frac=0.01,random_state=0))"]},{"cell_type":"markdown","metadata":{},"source":["Despite defect signals are related to a bigger dispersion, when you try to exclude this despersion seams like you are not able to separate them"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selected_columns = energy_col\n","\n","plot_scatter_matrix_FE(df, machine='M01', process='OP06', cols=selected_columns, sample_frac=0.1, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selected_columns = ['Z_D3', 'Z_D2', 'Y_D3', 'Y_D2', 'X_D3', 'X_Rolling Energy Entropy','Y_Rolling Energy Entropy','Z_Rolling Energy Entropy', 'Y_Rolling RMS']\n","\n","plot_scatter_matrix_FE(df, machine='M01', process='OP06', cols=selected_columns, sample_frac=0.1, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selected_columns = ['Z_D3', 'Z_D2', 'Y_D3', 'Y_D2', 'X_D3']\n","\n","plot_scatter_matrix_FE(df, machine='M02', process='OP06', cols=selected_columns, sample_frac=0.1, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selected_columns = variance_col\n","plot_scatter_matrix_FE(df, machine='M01', process='OP06', cols=selected_columns, sample_frac=0.1, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["selected_columns = original_col\n","plot_scatter_matrix_FE(df, machine='M01', process='OP06', cols=selected_columns, sample_frac=0.1, random_state=42)"]},{"cell_type":"markdown","metadata":{},"source":["# Gaussian Mixture Models"]},{"cell_type":"markdown","metadata":{},"source":["- A mixture model is a probabilistic model that represents a distribution as a mixture of simpler component distributions. In the context of clustering, a mixture model can be used to represent the distribution of data points as a mixture of Gaussian distributions, with each Gaussian representing a separate cluster.<br>\n","\n","References: <br>\n","[1] https://www.youtube.com/watch?v=5amKlNtIoT0 <br>\n","[2] https://behesht.medium.com/unsupervised-learning-clustering-using-gaussian-mixture-model-gmm-c788b280932b"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1 = df[df['Machine']=='M01']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_components = np.arange(1, 14)\n","\n","columns = ['X_axis', 'Y_axis', 'Z_axis','Z_D3', 'Z_D2', 'Y_D3', 'Y_D2', 'X_D3', 'X_Rolling Energy Entropy','Y_Rolling Energy Entropy','Z_Rolling Energy Entropy', 'Y_Rolling RMS']\n","\n","results = fit_gmm_evaluate(df1, columns, n_components, random_state=0,covariance_type = 'full')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["models = results['models']\n","bics = results['bics']\n","log_likelihoods = results['log_likelihoods']\n","davies_bouldin_indices = results['davies_bouldin_indices']\n","calinski_harabasz_indices = results['calinski_harabasz_indices']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(18, 10))\n","\n","\n","plt.subplot(2, 2, 1)\n","plt.plot(n_components, bics, label='BIC')\n","plt.title('Bayesian Information Criterion')\n","plt.xlabel('Number of components')\n","plt.ylabel('BIC')\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(n_components, log_likelihoods, label='Log Likelihood')\n","plt.title('Log Likelihood')\n","plt.xlabel('Number of components')\n","plt.ylabel('Log Likelihood')\n","\n","plt.subplot(2, 2, 3)\n","plt.plot(n_components[1:], davies_bouldin_indices[1:], label='Davies-Bouldin Index') # Lower is better\n","plt.title('Davies-Bouldin Index')\n","plt.xlabel('Number of components')\n","plt.ylabel('Davies-Bouldin Index')\n","\n","plt.subplot(2, 2, 4)\n","plt.plot(n_components[1:], calinski_harabasz_indices[1:], label='Calinski-Harabasz Index') # Higher is better\n","plt.title('Calinski-Harabasz Index')\n","plt.xlabel('Number of components')\n","plt.ylabel('Calinski-Harabasz Index')\n","\n","# plt.subplot(2, 3, 6)\n","# plt.plot(n_components[1:], silhouette_scores[1:], label='Silhouette Coefficient')\n","# plt.title('Silhouette Coefficient')\n","# plt.xlabel('Number of components')\n","# plt.ylabel('Silhouette Score')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gmm = GaussianMixture(2, covariance_type='full', random_state=0).fit(df1[columns])\n","# means = gmm.means_\n","# covariances = gmm.covariances_\n","\n","labels = gmm.predict(df1[columns])\n","\n","# Adding the label to the dataset with the TripNumber variable\n","df1['Clustering_Labels'] = labels\n","df1['Clustering_Labels'] = df1['Clustering_Labels'].astype('object')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accels_cols = ['Z_D3', 'Z_D2', 'Y_D3', 'Y_D2', 'X_D3', 'X_Rolling Energy Entropy','Y_Rolling Energy Entropy','Y_Rolling RMS']\n","\n","\n","plotly_scattermatrix(df=df1.sample(frac=0.10,random_state=0), cols=accels_cols,\n","                     color='Clustering_Labels', category_order={'Clustering_Labels':[0,1]},\n","                     width=1800, height=1100,\n","                     label_fontsize=18, legend_fontsize=26,\n","                     upload=False, filename=None \n","                     )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1['Clustering_Labels'] = labels\n","\n","def classify_labels_ua(value):\n","    if value in [1]: \n","        return 'Bad'\n","    else:\n","        return 'Normal'\n","\n","df1['Anomalies'] = df1['Clustering_Labels'].apply(classify_labels_ua)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_vars2 = ['Anomalies','Unique_Code','Z_D3', 'Z_D2', 'Y_D3', 'Y_D2', 'X_D3','Machine','Z_Rolling Energy Entropy', 'Y_Rolling RMS']\n","visualize_with_hiplot(df1[plot_vars2].sample(frac=0.01,random_state=0))"]},{"cell_type":"markdown","metadata":{},"source":["- https://ravindranathsawane.medium.com/spectral-clustering-algorithm-b469938a8841\n","- https://github.com/koaning/drawdata?tab=readme-ov-file"]},{"cell_type":"markdown","metadata":{},"source":["What if we first try to apply PCA to separate the data?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = ['X_axis', 'Y_axis', 'Z_axis','Z_D3', 'Z_D2', 'Y_D3', 'Y_D2', 'X_D3', 'X_Rolling Energy Entropy','Y_Rolling Energy Entropy','Z_Rolling Energy Entropy', 'Y_Rolling RMS']\n","\n","X = df1[features]\n","X.reset_index(drop=True, inplace=True)\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_scaled = pd.DataFrame(X_scaled, columns=features)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_pca_cumulative_variance(X_scaled, n_components=None):\n","    \"\"\"\n","    Fit a PCA model to the given scaled data and plot the cumulative explained variance.\n","\n","    Parameters:\n","    - X_scaled: The scaled input data (e.g., a NumPy array or DataFrame).\n","    - n_components: Number of PCA components to consider (default is None, which means all).\n","    \"\"\"\n","    # Initialize PCA model\n","    pca = PCA(n_components=n_components)\n","    \n","    # Fit PCA model to the scaled data\n","    pca.fit(X_scaled)\n","    \n","    # Calculate the cumulative explained variance\n","    cumulative_variance = pca.explained_variance_ratio_.cumsum()\n","    \n","    # Create a plot of the cumulative explained variance\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n","    plt.xlabel('Number of Components')\n","    plt.ylabel('Cumulative Explained Variance')\n","    plt.title('PCA - Cumulative Variance Explained by Components')\n","    plt.grid(True)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_pca_cumulative_variance(X_scaled)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_components = 8\n","pca = PCA(n_components=n_components) \n","X_pca = pca.fit_transform(X_scaled)\n","explained_var = pca.explained_variance_ratio_\n","print(\"Explained variance for each component:\", explained_var)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["columns = [f'PC{i+1}' for i in range(n_components)]\n","\n","\n","pca_df = pd.DataFrame(data = X_pca[:, :8], columns=columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df1.reset_index(drop=True,inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_df['Label'] = df1['Label']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_df['Label'] = pca_df['Label'].astype('str')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accels_cols = columns\n","\n","\n","plotly_scattermatrix(df=pca_df.sample(frac=0.10,random_state=0), cols=accels_cols,\n","                     color='Label', category_order={'Label':[0,1]},\n","                     width=1800, height=1100,\n","                     label_fontsize=18, legend_fontsize=26,\n","                     upload=False, filename=None \n","                     )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_df['Unique_Code'] = df1['Unique_Code']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_components = np.arange(1, 15)\n","columns = ['PC1','PC2','PC3','PC4','PC5','PC6','PC8','PC8']\n","\n","results = fit_gmm_evaluate(pca_df, columns, n_components, random_state=0,covariance_type = 'full')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["models = results['models']\n","bics = results['bics']\n","log_likelihoods = results['log_likelihoods']\n","davies_bouldin_indices = results['davies_bouldin_indices']\n","calinski_harabasz_indices = results['calinski_harabasz_indices']"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(18, 10))\n","\n","\n","plt.subplot(2, 2, 1)\n","plt.plot(n_components, bics, label='BIC')\n","plt.title('Bayesian Information Criterion')\n","plt.xlabel('Number of components')\n","plt.ylabel('BIC')\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(n_components, log_likelihoods, label='Log Likelihood')\n","plt.title('Log Likelihood')\n","plt.xlabel('Number of components')\n","plt.ylabel('Log Likelihood')\n","\n","plt.subplot(2, 2, 3)\n","plt.plot(n_components[1:], davies_bouldin_indices[1:], label='Davies-Bouldin Index') # Lower is better\n","plt.title('Davies-Bouldin Index')\n","plt.xlabel('Number of components')\n","plt.ylabel('Davies-Bouldin Index')\n","\n","plt.subplot(2, 2, 4)\n","plt.plot(n_components[1:], calinski_harabasz_indices[1:], label='Calinski-Harabasz Index') # Higher is better\n","plt.title('Calinski-Harabasz Index')\n","plt.xlabel('Number of components')\n","plt.ylabel('Calinski-Harabasz Index')\n","\n","# plt.subplot(2, 3, 6)\n","# plt.plot(n_components[1:], silhouette_scores[1:], label='Silhouette Coefficient')\n","# plt.title('Silhouette Coefficient')\n","# plt.xlabel('Number of components')\n","# plt.ylabel('Silhouette Score')\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["columns"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gmm = GaussianMixture(4, covariance_type='full', random_state=0).fit(pca_df[columns])\n","# means = gmm.means_\n","# covariances = gmm.covariances_\n","\n","labels = gmm.predict(pca_df[columns])\n","\n","# Adding the label to the dataset with the TripNumber variable\n","pca_df['Clustering_Labels'] = labels\n","pca_df['Clustering_Labels'] = pca_df['Clustering_Labels'].astype('object')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["accels_cols = columns\n","\n","\n","plotly_scattermatrix(df=pca_df.sample(frac=0.10,random_state=0), cols=accels_cols,\n","                     color='Clustering_Labels', category_order={'Clustering_Labels':[0,1]},\n","                     width=1800, height=1100,\n","                     label_fontsize=18, legend_fontsize=26,\n","                     upload=False, filename=None \n","                     )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_df['Clustering_Labels'] = labels\n","\n","def classify_labels_ua(value):\n","    if value in [3]: \n","        return 'Bad'\n","    else:\n","        return 'Normal'\n","\n","pca_df['Anomalies'] = pca_df['Clustering_Labels'].apply(classify_labels_ua)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plot_vars2 = ['Anomalies','Unique_Code','PC1','PC2','PC3','PC4','PC5','PC6','PC8','PC8']\n","visualize_with_hiplot(pca_df[plot_vars2].sample(frac=0.01,random_state=0))"]},{"cell_type":"markdown","metadata":{},"source":["Maybe because of the bahavior of the data, GMM is not the most indicated algorithm fot this problem"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df2['Label'] = df2['Label'].astype('str')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.scatter(df2[::100], x='Z_Rolling Impulse Factor', y='Z_Rolling Margin Factor', color='Label',\n","                 labels={\n","                     'Z_axis': 'Z_Rolling Impulse Factor',\n","                     'Y_axis': 'Z_Rolling Margin Factor',\n","                     'Label': 'Label'\n","                 },\n","                 title='IF and MF (Z_axis) axis for M01 - OP06')\n","\n","fig.update_layout(width=800, height=600)\n","\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.scatter(df2[::100], x='Y_Rolling Impulse Factor', y='Z_Rolling Variance', color='Label',\n","                 labels={\n","                     'Z_axis': 'Z_Rolling Impulse Factor',\n","                     'Y_axis': 'Z_Rolling Margin Factor',\n","                     'Label': 'Label'\n","                 },\n","                 title='IF and MF (Z_axis) axis for M01 - OP06')\n","\n","fig.update_layout(width=800, height=600)\n","\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = ['X_Rolling Mean', 'X_Rolling Median',\n","       'X_Rolling Variance', 'X_Rolling Skewness', 'X_Rolling Impulse Factor',\n","       'X_Rolling Margin Factor', 'Y_Rolling Mean', 'Y_Rolling Median',\n","       'Y_Rolling Variance', 'Y_Rolling Skewness','Y_Rolling Impulse Factor',\n","       'Y_Rolling Margin Factor', 'Z_Rolling Mean',\n","       'Z_Rolling Median','Z_Rolling Variance', 'Z_Rolling Skewness', 'Z_Rolling Crest Factor',\n","        'Z_Rolling Impulse Factor','Z_Rolling Margin Factor']\n","\n","X = df[features]\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca = PCA(n_components=2) \n","X_pca = pca.fit_transform(X_scaled)\n","PCA()\n","\n","explained_var = pca.explained_variance_ratio_\n","print(\"Explained variance for each component:\", explained_var)"]},{"cell_type":"markdown","metadata":{},"source":["PCA it's not good to describe this dataset - but let's just take a look on how it goes"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_df = pd.DataFrame(data = X_pca[:, :2], columns = ['PC1', 'PC2'])\n","pca_df['Label'] = df['Label'].values \n","\n","pca_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pca_df['Label'] = pca_df['Label'].astype('str')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.scatter(pca_df[::100], x='PC1', y='PC2', color='Label',\n","                 labels={\n","                     'PC1': 'PC1',\n","                     'PC2': 'PC1'\n","                 },\n","                 title='PCA')\n","fig.show()"]},{"cell_type":"markdown","metadata":{},"source":["# U-MAP"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["features = ['X_Rolling Variance', 'X_Rolling Skewness', 'X_Rolling Impulse Factor',\n","       'X_Rolling Margin Factor', 'Y_Rolling Variance', 'Y_Rolling Skewness','Y_Rolling Impulse Factor',\n","       'Y_Rolling Margin Factor', 'Z_Rolling Variance', 'Z_Rolling Skewness', 'Z_Rolling Crest Factor',\n","        'Z_Rolling Impulse Factor','Z_Rolling Margin Factor']\n","\n","X = df[features]\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X.iloc[::100])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target = df['Label'].iloc[::100].values  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["reducer = umap.UMAP(random_state=42)\n","X_umap = reducer.fit_transform(X_scaled, y=target)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["embedding = reducer.embedding_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib.colors import BoundaryNorm\n","\n","# Criando uma figura e um subplot\n","fig, ax = plt.subplots(1, figsize=(8, 6))\n","\n","# Define os limites para a barra de cores (inclui um limite extra para garantir a cobertura de todos os valores)\n","boundaries = [0, 0.5, 1]\n","norm = BoundaryNorm(boundaries, ncolors=256, clip=True)\n","\n","# Cria o gráfico de dispersão\n","scatter = ax.scatter(*embedding.T, s=0.1, c=target, cmap='Spectral', norm=norm, alpha=1.0)\n","\n","# Adiciona a barra de cores com limites discretos\n","cbar = plt.colorbar(scatter, ax=ax, ticks=[0, 1])\n","cbar.set_label('Target')\n","\n","# Mostra o gráfico\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Wavelet Features"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["path_to_dataset = \"../3.Feature_Engineering/Datasets/DF_Features.parquet\"\n","df = pd.read_parquet(path_to_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["df.columns"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["# Visualy selected features - free-style\n","\n","selected_columns = ['X_D1','X_D2', 'X_D3', 'X_A3', 'Y_D1', 'Y_D2', 'Y_D3', 'Y_A3', 'Z_D1', 'Z_D2',\n","       'Z_D3', 'Z_A3','Label']\n","df2 = df[selected_columns]"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["df2['Label'] = df2['Label'].astype('str')"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["fig = px.scatter(df2[::100], x='Y_D3', y='Z_D3', color='Label',\n","                 labels={\n","                     'X_axis': 'Y_D3',\n","                     'Y_axis': 'Z_D3',\n","                     'Label': 'Label'\n","                 },\n","                 title='D3 for M01 - OP06')\n","\n","fig.update_layout(width=800, height=600)\n","\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["scaler = StandardScaler()\n","features = ['X_D1','X_D2', 'X_D3', 'X_A3', 'Y_D1', 'Y_D2', 'Y_D3', 'Y_A3', 'Z_D1', 'Z_D2',\n","       'Z_D3', 'Z_A3', 'X_Rolling Variance', 'X_Rolling Skewness', 'X_Rolling Impulse Factor',\n","       'X_Rolling Margin Factor', 'Y_Rolling Variance', 'Y_Rolling Skewness','Y_Rolling Impulse Factor',\n","       'Y_Rolling Margin Factor', 'Z_Rolling Variance', 'Z_Rolling Skewness', 'Z_Rolling Crest Factor',\n","        'Z_Rolling Impulse Factor','Z_Rolling Margin Factor', 'X_Rolling Median', 'Y_Rolling Median', 'Z_Rolling Median']\n","X = df[features]\n","X_scaled = scaler.fit_transform(X)"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["pca = PCA(n_components=2) \n","X_pca = pca.fit_transform(X_scaled)\n","PCA()\n","\n","explained_var = pca.explained_variance_ratio_\n","print(\"Explained variance for each component:\", explained_var)"]},{"cell_type":"markdown","metadata":{},"source":["Low explained variance for the first 2 components"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["pca_df = pd.DataFrame(data = X_pca[:, :2], columns = ['PC1', 'PC2'])\n","pca_df['Label'] = df2['Label'].values \n","\n","pca_df"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["fig = px.scatter(pca_df[::100], x='PC1', y='PC2', color='Label',\n","                 labels={\n","                     'PC1': 'PC1',\n","                     'PC2': 'PC1'\n","                 },\n","                 title='PCA')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["pca_df['Label'] = pca_df['Label'].astype('str')"]},{"cell_type":"markdown","metadata":{},"source":["### U-MAP"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["X = df[features]\n","\n","scaler = StandardScaler()\n","X_scaled = scaler.fit_transform(X.iloc[::100])"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["#target = df['Label'].iloc[::100].values  \n","\n","reducer = umap.UMAP(random_state=40, n_neighbors=10)\n","X_umap = reducer.fit_transform(X_scaled,)\n","\n","embedding = reducer.embedding_"]},{"cell_type":"code","execution_count":null,"metadata":{"metadata":{}},"outputs":[],"source":["fig, ax = plt.subplots(1, figsize=(8, 6))\n","\n","target = df['Label'].iloc[::100].values  \n","\n","# Define os limites para a barra de cores (inclui um limite extra para garantir a cobertura de todos os valores)\n","boundaries = [0, 0.5, 1]\n","norm = BoundaryNorm(boundaries, ncolors=256, clip=True)\n","\n","# Cria o gráfico de dispersão\n","scatter = ax.scatter(*embedding.T, s=0.1, c=target, cmap='Spectral', norm=norm, alpha=1.0)\n","\n","# Adiciona a barra de cores com limites discretos\n","cbar = plt.colorbar(scatter, ax=ax, ticks=[0, 1])\n","cbar.set_label('Target')\n","\n","# Mostra o gráfico\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["umap_data = pd.DataFrame(X_umap, columns=['UMAP1', 'UMAP2'])\n","umap_data['Label'] = df['Label'].iloc[::100].values  \n","umap_data['Label'] = umap_data['Label'].astype('str')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = px.scatter(umap_data, x='UMAP1', y='UMAP2', color='Label', title='UMAP Projection - 2d')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_scaled"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["num_clusters = 2\n","km = KMeans(n_clusters=num_clusters,random_state=10,n_init=1) # n_init, number of times the K-mean algorithm will run\n","km.fit(X_scaled[features])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["labels_pred = km.labels_\n","\n","# Plotting PCA results with K-means labels\n","plt.figure(figsize=(14, 6))\n","\n","# First subplot with K-means labels\n","plt.subplot(1, 2, 1)\n","for i in range(num_clusters):\n","    plt.scatter(X_pca[labels_pred == i, 0], X_pca[labels_pred == i, 1], label=f'Cluster {i}')\n","plt.title('PCA with K-means Clusters')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.legend()\n","\n","# Second subplot with existing labels\n","plt.subplot(1, 2, 2)\n","# Assuming the existing labels are categorical and not numeric\n","unique_labels = df['Existing_Labels'].unique()\n","for label in unique_labels:\n","    plt.scatter(X_pca[df['Existing_Labels'] == label, 0], X_pca[df['Existing_Labels'] == label, 1], label=f'Label {label}')\n","plt.title('PCA with Existing Labels')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.legend()\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["K-means clustering is one of the most simple clustering algorithms.  One of the limitations is that it depends on the starting point of the clusters, and the number of clusters need to be defined beforehand.\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Cluster starting points\n","Let's start by creating a simple dataset.\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Let's now group this data into two clusters.  We will use two different random states to initialize the algorithm. Settign a the __[random state](https://numpy.org/doc/stable/reference/random/legacy.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01#numpy.random.RandomState)__ variable is useful for testing and allows us to seed the randomness (so we get the same results each time).\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Clustering with a random state of 10:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["num_clusters = 2\n","km = KMeans(n_clusters=num_clusters,random_state=10,n_init=1) # n_init, number of times the K-mean algorithm will run\n","km.fit(X)\n","display_cluster(X,km,num_clusters)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Clustering with a random state of 20:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["km = KMeans(n_clusters=num_clusters,random_state=20,n_init=1)\n","km.fit(X)\n","display_cluster(X,km,num_clusters)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["## Question:\n","\n","Why are the clusters different when we run  the K-means twice?\n","\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["It's because the starting points of the cluster centers have an impact on where the final clusters lie.  The starting point of the clusters is controlled by the random state.\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Determining optimium number of clusters\n","\n","Let's create a new dataset that visually consists on a few clusters and try to group them.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["n_samples = 1000\n","n_bins = 4  \n","centers = [(-3, -3), (0, 0), (3, 3), (6, 6)]\n","X, y = make_blobs(n_samples=n_samples, n_features=2, cluster_std=1.0,\n","                  centers=centers, shuffle=False, random_state=42)\n","display_cluster(X)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["How many clusters do you observe?\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Let's run K-means with seven clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["num_clusters = 7\n","km = KMeans(n_clusters=num_clusters)\n","km.fit(X)\n","display_cluster(X,km,num_clusters)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Now let's re-run the algorithm with four clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["num_clusters = 4\n","km = KMeans(n_clusters=num_clusters)\n","km.fit(X)\n","display_cluster(X,km,num_clusters)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Should we use four or seven clusters?  \n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["- In this case it may be visually obvious that four clusters is better than seven.  \n","- This is because we can easily view the data in two dimensional space.  \n","- However, real world data usually has more than two dimensions.  \n","- A dataset with a higher dimensional space is hard to visualize.  \n","- A way of solving this is to plot the **inertia** \n","\n","**inertia**: (sum of squared error between each point and its cluster center) as a function of the number of clusters. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["km.inertia_"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Problem 1:\n","\n","Write code that calculates the inertia for 1 to 10 clusters, and plot the inertia as a function of the number of clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["### BEGIN SOLUTION\n","inertia = []\n","list_num_clusters = list(range(1,11))\n","for num_clusters in list_num_clusters:\n","    km = KMeans(n_clusters=num_clusters)\n","    km.fit(X)\n","    inertia.append(km.inertia_)\n","    \n","plt.plot(list_num_clusters,inertia)\n","plt.scatter(list_num_clusters,inertia)\n","plt.xlabel('Number of Clusters')\n","plt.ylabel('Inertia');\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Where does the elbow of the curve occur?\n","\n","What do you think the inertia would be if you have the same number of clusters and data points?\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Clustering Colors from an Image\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["The next few exercises use an image of bell peppers. Let's start by loading it:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["img = plt.imread('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0187EN-SkillsNetwork/labs/module%201/images/peppers.jpg', format='jpeg')\n","plt.imshow(img)\n","plt.axis('off')"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["img.shape"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["The image above has 480 pixels in height and 640 pixels in width.  Each pixel has 3 values that represent how much red, green and blue it has. Below you can play with different combinations of RGB to create different colors. In total, you can create $256^3 = 16,777,216$ unique colors.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"slide"}},"outputs":[],"source":["# assign values for the RGB.  Each value should be between 0 and 255\n","R = 35\n","G = 95\n","B = 131\n","plt.imshow([[np.array([R,G,B]).astype('uint8')]])\n","plt.axis('off')"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["First we will reshape the image into a table that has a pixel per row and each column represents the red, green and blue channel.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["img_flat = img.reshape(-1, 3)\n","img_flat[:5,:]"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Since there are 480x640 pixels we get 307,200 rows! \n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["img_flat.shape"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Let's run K-means with 8 clusters.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["kmeans = KMeans(n_clusters=8, random_state=0).fit(img_flat)"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["Now let's replace each row with its closest cluster center.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["img_flat2 = img_flat.copy()\n","\n","# loops for each cluster center\n","for i in np.unique(kmeans.labels_):\n","    img_flat2[kmeans.labels_==i,:] = kmeans.cluster_centers_[i]"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"fragment"}},"source":["We now need to reshape the data from 307,200 x 3 to 480 x 640 x 3\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["img2 = img_flat2.reshape(img.shape)\n","plt.imshow(img2)\n","plt.axis('off');"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Problem 2:\n","Write a function that receives the image and number of clusters (k), and returns (1) the image quantized into k colors, and (2) the inertia.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["### BEGIN SOLUTION\n","def image_cluster(img, k):\n","    img_flat = img.reshape(img.shape[0]*img.shape[1],3)\n","    kmeans = KMeans(n_clusters=k, random_state=0).fit(img_flat)\n","    img_flat2 = img_flat.copy()\n","\n","    # loops for each cluster center\n","    for i in np.unique(kmeans.labels_):\n","        img_flat2[kmeans.labels_==i,:] = kmeans.cluster_centers_[i]\n","        \n","    img2 = img_flat2.reshape(img.shape)\n","    return img2, kmeans.inertia_\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Problem 3:\n","\n","Call the function for k between 2 and 20, and draw an inertia curve. What is the optimum number of clusters?\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["### BEGIN SOLUTION\n","k_vals = list(range(2,21,2))\n","img_list = []\n","inertia = []\n","for k in k_vals:\n","#    print(k)\n","    img2, ine = image_cluster(img,k)\n","    img_list.append(img2)\n","    inertia.append(ine)  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot to find optimal number of clusters\n","plt.plot(k_vals,inertia)\n","plt.scatter(k_vals,inertia)\n","plt.xlabel('k')\n","plt.ylabel('Inertia');\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["Sometimes, the elbow method does not yield a clear decision (for example, if the elbow is not clear and sharp, or is ambiguous).  In such cases, alternatives such as the [silhouette coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork821-2023-01-01) can be helpful.\n"]},{"cell_type":"markdown","metadata":{"slideshow":{"slide_type":"slide"}},"source":["### Problem 4:\n","Plot in a grid all the images for the different k values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"slideshow":{"slide_type":"fragment"}},"outputs":[],"source":["### BEGIN SOLUTION\n","plt.figure(figsize=[10,20])\n","for i in range(len(k_vals)):\n","    plt.subplot(5,2,i+1)\n","    plt.imshow(img_list[i])\n","    plt.title('k = '+ str(k_vals[i]))\n","    plt.axis('off');\n","### END SOLUTION"]},{"cell_type":"markdown","metadata":{},"source":["---\n","### Machine Learning Foundation (C) 2020 IBM Corporation\n"]}],"metadata":{"kernelspec":{"display_name":"ETL","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.0"}},"nbformat":4,"nbformat_minor":4}
